---
title: 'Sources of variability (MixR)'
author: 'Ben Whalley'
date: "Feb 2021"
bibliography: [references.bib]
biblio-style: apa6
link-citations: yes
output:
  webex::html_clean
---



```{r, include=F, echo=F}
library(webex)
library(tidyverse)

knitr::opts_chunk$set(echo = TRUE,
  collapse=TRUE,
  comment=NA,
  cache = TRUE,
  message=FALSE)
```


### In brief

> Many psychological studies, both applied and experimental, make repeated measurements
> of some kind. In this first session we discuss the implications of making repeating measues, 
> and the concept of partitioning variance, attributing it to different groupings in our data.


```{r, echo=F, include=F}
knitr::opts_chunk$set(echo = TRUE, collapse=TRUE, cache=TRUE, comment=">", message=FALSE)
library(tidyverse)
library(webex)
library(pander)
theme_set(theme_minimal())
```



## Class activities


:::{.exercise}

- [Schools sampling strategy](handouts/schools-sampling-activity.pdf)

- [Schools plot activity](handouts/schools-plot-activity.pdf)

- [Code/output from the live demo](01-school-leaver-demo.html)

:::

<!-- A worked example

To put some numbers to the claim above:

Imagine we were studying a medium effect size (D=.5), in a situation where 15% of the variance in outcomes was explained by the school a child attended.

We want to achieve power of 80% for alpha = .05.

In this case, increasing the number of schools sampled from 20 to 21 would reduce the total N (total number of students) from 111 students to 105.

That is, we sample fewer students, but gain statistical power.
 -->



### Scottish School Leavers

> [5.1.txt](data/cmm/5.1.txt) contains data from the Scottish School Leavers Survey (SSLS), a nationally representative survey of young people. The file has data from seven cohorts of young people collected in the first sweep of the study, carried out at the end of the final year of compulsory schooling (aged 16-17) when most sample members had taken Standard grades.

These data form part of the Bristol CMM multilevel modelling course, which is free to students in the UK at <https://www.cmm.bris.ac.uk/lemma/>


```{r readssldata}
schools <- read_csv('data/cmm/5.1.txt')
```



:::{.exercise}

- Read in the scottish school leaver data ([this file](data/cmm/5.1.txt))

- Use ggplot to visualise how much variation in `score` there is within and between schools. You might need to filter/subset the data in some way to avoid being overwhelmed.


`r hide("Hints for the plot")`

One option would be a boxplot where `schoolid` is the x axis. Another would be to use `stat_summary` which would show the mean and SE for each school.

`r unhide()`

- Use `lmer` (load the `lmerTest` library) to fit a random intercepts model with `score` as the outcome. Remember that `~1` will add an an intercept even if there are no predictors.

- 


:::




## Word-naming experiment


In the PSYC753 [workshop on repeated measures anove we saw a dataset from a word naming study](https://benwhalley.github.io/datafluency/workshop-repeated-measures.html)

In this example we show how mixed models (using lmer) can replicate the Repeat Measure Anova
analysis we ran in that workshop.

You can load these data as follows:

```{r}
words <- read_csv("https://raw.githubusercontent.com/ajwills72/rminr/master/src/wordnaming2.csv")
```


Unlike in the RM Anova workshop we can use these data as-is, in long-from. 
We don't need to do any aggregation.


:::{.exercise}

- Fit a random-intercepts model for `rt`. Allow intercepts to vary by `subj`. Don't include any other predictors for the moment.


`r hide("Show hint")`

`rt` will be the outcome, to the left of the tilde (`~`)

To fit the intercept only model you write `~1` instead of adding predictors

The random effects are defined by writing `+ (1|groupingvariable)` 

`r unhide()`


- Save the model to a variable and use the `VarCorr` function on it. What proportion of the variation in reaction times is explained between-subjects, and how much is unexplained (within subject)? 


`r hide("Show hint")`

Remember to convert the `VarCorr` output to a dataframe to show the variances (not the standard deviations). You can use these to calculate the VPC

`r unhide()`



- Ad the `medit` and `congru` variables as predictors. Allow them to interact. Save the model and use the `anova` function on the saved model. Is the interaction between `medit` and `congru` statistically significant?


`r hide("Show hint")`

The relevant line in the Anova output is:

```
              Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    
medit:congru 3521012 3521012     1 16519 532.587 < 2.2e-16 ***
```

So yes, the interaction between them is statistically significant (the p value is tiny and way less than .001).

`r unhide()`



`r hide("Show all the code to produce the answers")`

```{r}
library(lmerTest)

words.subset <- words %>% 
  filter(medit != "relax") %>% 
  filter(congru != "neutral") 

# run the model
words.subj.interceptonly <- lmer(rt ~ 1 + (1|subj), data=words.subset)

# calculate VPC (it's about 37% between, 63 within/unexplained)
words.subj.interceptonly %>% 
  VarCorr() %>%
  as_data_frame() %>%
  mutate(vpc = vcov/sum(vcov) * 100) %>% 
  pander()

words.subj.group <- lmer(rt ~ medit * congru + (1|subj), data=words.subset)

# test effect of congruency and interaction with meditation group assignment
anova(words.subj.group)

```

`r unhide()`



:::





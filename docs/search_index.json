[
["random-effects.html", "Random effects", " Random effects In brief In this session we specify more complex random effects, including examples from experimental data with multiple sources of variation. We use visualisation techniques to explore the concept of ‘shrinkage’. "],
["politeness.html", "Politeness", " Politeness Winter &amp; Grawunder, 2012 describe a study of the pitch (frequency) of individuals’ speech when they recorded different phrases (scenarios). The scenarios differed in whether they required ‘politeness’ or were more informal in nature. A reduced version of this dataset is also described and analysed in a mixed-models tutorial (see http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf). The data are available online and can be read from this URL: # this previously taken from http://www.bodowinter.com/tutorial/politeness_data.csv but that link now a 404 politeness &lt;- read_csv(&quot;https://raw.githubusercontent.com/opetchey/BIO144/master/3_datasets/politeness_data.csv&quot;) Make a plot showing how average levels of frequency differs between individuals. Show me the plot politeness %&gt;% ggplot(aes(subject, frequency)) + geom_boxplot() You could also use stat_summary() instead of geom_boxplot() to get a point range plot. Make a plot showing how frequency differs between scenarios Show me the plot politeness %&gt;% ggplot(aes(factor(scenario), frequency)) + geom_boxplot() Based on the plots, which do you think accounts for more variation in frequency: subject or scenario? Show answer It looks like subjects vary more in frequency than do scenarios, but there appears to be variation attributable to both variables. Fit a random intercepts model to the data, allowing for variation between subjects. Show me that model (polite.ri.subject &lt;- lmer(frequency ~ attitude + (1|subject), data=politeness)) ## Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] ## Formula: frequency ~ attitude + (1 | subject) ## Data: politeness ## REML criterion at convergence: 804.7023 ## Random effects: ## Groups Name Std.Dev. ## subject (Intercept) 63.10 ## Residual 29.17 ## Number of obs: 83, groups: subject, 6 ## Fixed Effects: ## (Intercept) attitudepol ## 202.59 -19.38 What other commands should I run to interpret the model? You might then also want to look at the regression coefficients, Anova table, and tests of random effects: # regression coefficients polite.ri.subject %&gt;% summary %&gt;% coef() ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 202.58810 26.151105 5.150024 7.746827 0.0005008339 ## attitudepol -19.37584 6.406962 76.002550 -3.024185 0.0033990395 # anova-table anova(polite.ri.subject) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## attitude 7782.9 7782.9 1 76.003 9.1457 0.003399 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # tests of random effects ranova(polite.ri.subject) ## ANOVA-like table for random-effects: Single term deletions ## ## Model: ## frequency ~ attitude + (1 | subject) ## npar logLik AIC LRT Df Pr(&gt;Chisq) ## &lt;none&gt; 4 -402.35 812.7 ## (1 | subject) 3 -457.15 920.3 109.6 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Fit a second random intercepts model, allowing for variation in both subjects and between the different scenarios. Use to VarCorr function to estimate how much variation (what percentage of the total) was between subjects, and what percentage was between scenarios. Show me that model polite.ri.both &lt;- lmer(frequency ~ attitude + (1|subject) + (1|scenario), data=politeness) ranova(polite.ri.both) ## ANOVA-like table for random-effects: Single term deletions ## ## Model: ## frequency ~ attitude + (1 | subject) + (1 | scenario) ## npar logLik AIC LRT Df Pr(&gt;Chisq) ## &lt;none&gt; 5 -396.73 803.45 ## (1 | subject) 4 -457.15 922.30 120.851 1 &lt; 2.2e-16 *** ## (1 | scenario) 4 -402.35 812.70 11.249 1 0.0007968 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How much variance was attributable to subjects or scenarios, compared with the total variance? Show ne how to calculate this Remember to convert the VarCorr output to a dataframe to see the variance (rather than standard deviations) in the vcov column. You can then use mutate to calculate the within/betwen ratio (the ICC). VarCorr(polite.ri.both) %&gt;% as.tibble() %&gt;% select(grp, vcov) %&gt;% mutate(icc = vcov / sum(vcov)) %&gt;% pander() grp vcov icc scenario 219 0.04488 subject 4015 0.8227 Residual 646 0.1324 Add random slopes to the model above. Allow the effect of attitude (whether the scenario was polite or informal) to differ between subjects, and also to differ between the different example scenarios sampled in this study. Show me that model formula frequency ~ attitude + (attitude|subject) + (attitude|scenario) ## frequency ~ attitude + (attitude | subject) + (attitude | scenario) Use ranova to test whether the random slopes were significant predictors of variance in frequency. Show me how First run the model: polite.rslopes &lt;- lmer(frequency ~ attitude + (attitude|subject) + (attitude|scenario), data=politeness) If you see a message saying boundary (singular) fit: see ?isSingular don’t worry for now. This IS actually quite important, but we will discuss it in more detail in the next session. And then use ranova: ranova(polite.rslopes) ## ANOVA-like table for random-effects: Single term deletions ## ## Model: ## frequency ~ attitude + (attitude | subject) + (attitude | scenario) ## npar logLik AIC LRT Df Pr(&gt;Chisq) ## &lt;none&gt; 9 -395.76 809.51 ## attitude in (attitude | subject) 7 -396.55 807.11 1.59356 2 0.4508 ## attitude in (attitude | scenario) 7 -395.95 805.89 0.38097 2 0.8266 Both of the Pr(&gt;Chisq) values (the p values for the chi squared test) are non-significant. Use anova to test the same thing: whether adding the random slope for attitude within scenario or subject improved the model. Show me how First run two models, with and without the random slope you want to test: model.without &lt;- lmer(frequency ~ attitude + (1|subject) + (attitude|scenario), data=politeness) model.with &lt;- lmer(frequency ~ attitude + (attitude|subject) + (attitude|scenario), data=politeness) And then use anova: anova(model.with, model.without) ## Data: politeness ## Models: ## model.without: frequency ~ attitude + (1 | subject) + (attitude | scenario) ## model.with: frequency ~ attitude + (attitude | subject) + (attitude | scenario) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## model.without 7 820.88 837.81 -403.44 806.88 ## model.with 9 823.32 845.09 -402.66 805.32 1.5578 2 0.4589 You should notice that the p value for the test has the same number of degrees of freedom, and the same value (very slightly different due to rounding errors). The Pr(&gt;Chisq) is again non-significant, indicating that the random slope within subject doesn’t explain additional variance. If a random slopes model is not ‘significantly’ better than a similar model which does not include the random slope term, is there any reason why we might still prefer it, and use the ‘full’ model to base our inference on? Show answer Yes - simulation studies, including Barr et al 2013 suggest that a ‘maximal’ model is likely to be more conservative than a model which excludes non-significant random effects terms. How might mixed models make the problem of ‘researcher degrees of freedom’ worse? What effect might this have? How can it be avoided? Show answer Mixed models provide many more possible ‘ways to do it’. In addition to different fixed-effects specifications we can now also have many different random effects specifications. This increases degrees of freedom in the analysis, and makes it even more important to pre-specify analyses. "],
["further-reading.html", "Further reading", " Further reading https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.2041-210x.2012.00261.x Barr et al go into more detail on why mixed models are preferable to RM anova, and this paper will be a useful reference for future sessions too: Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 255-278. This paper expands on the original ‘keep it maximal’ paper, covering situations with interactions between within factors: Barr, D. J. (2013). Random effects structure for testing interactions in linear mixed-effects models. Frontiers in psychology, 4, 328. See also: Eager, C., &amp; Roy, J. (2017). Mixed effects models are sometimes terrible. arXiv preprint arXiv:1701.04858. "]
]

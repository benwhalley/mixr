[
["index.html", "MixR: Mixed Models in R ", " MixR: Mixed Models in R Ben Whalley image: needpix This short course aims to extend your existing skills, and give you confidence when working with and analysing repeated measures data. We cover examples typical experimental and applied studies in psychology. We will cover repeat measures Anova designs, mixed-effects models, touch on Bayesian parameter estimation for mixed models, and learn how to test specific hypotheses from mixed models. Approach In common with all modules at Plymouth, we try to avoid the ‘bag of tricks’ approach to teaching research methods and try to integrate new skills into a broader approach to collecting and using data. Our workflow is inspired by Wickham’s model for data science: Wickham, 2017: Wickham’s model of a data science workflow In this option we do teach specific techniques, but the aim is always to help you integrate this new knowledge into your own research practice. Access to R Throughout the module we use R for data processing and analysis. If you are taking this course at Plymouth University, the easiest way to run the code examples here is to the school’s RStudio Server. Login to your account on the server here To get an account on the server, or reset a password, contact the Psychology technical office Installing at home If you want to install R on your own machine, instructions are available here: https://github.com/plymouthPsychology/installR/ Be sure to install the recommended packages or the examples given here won’t work. License All content on this site distributed under a Creative Commons licence. CC-BY-SA 4.0. "],
["repeated-measures.html", "Repeated measures", " Repeated measures Image: Jeff Peachey In brief Many psychological studies, both applied and experimental, make repeated measurements of some kind. Dealing with these with confidence is the focus of this option. In this first session on repeated measures data we introduce the idea of partitioning variance, and attributing it to different groupings in our data. We discuss how this is related to statistical power, and fit the same pre-post RCT data with Ancova and a ‘mixed’ model. At the end of this session you should be able to identify tradeoffs in sampling strategies for repeat measures studies and understand the drivers for dealing with repeat measures data with appropriate techniques. Slides for today are here: repeated-measures.pptx. "],
["exercises.html", "Exercises", " Exercises Scottish School Leavers 5.1.txt contains data from the Scottish School Leavers Survey (SSLS), a nationally representative survey of young people. The file has data from seven cohorts of young people collected in the first sweep of the study, carried out at the end of the final year of compulsory schooling (aged 16-17) when most sample members had taken Standard grades. These data form part of the Bristol CMM multilevel modelling course, which is free to students in the UK at https://www.cmm.bris.ac.uk/lemma/ schools &lt;- read_csv(&#39;data/cmm/5.1.txt&#39;) schools %&gt;% skimr::skim() Table 1: Data summary Name Piped data Number of rows 33988 Number of columns 9 _______________________ Column type frequency: numeric 9 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist caseid 0 1 18465.74 11435.16 1 8531.75 17318.5 29428.25 38192 ▇▇▇▅▇ schoolid 0 1 254.36 149.76 1 123.00 256.0 386.00 511 ▇▇▇▇▇ score 0 1 31.09 17.31 0 19.00 33.0 45.00 75 ▅▆▇▅▁ cohort90 0 1 0.28 5.35 -6 -4.00 -2.0 6.00 8 ▇▃▃▁▇ female 0 1 0.53 0.50 0 0.00 1.0 1.00 1 ▇▁▁▁▇ sclass 0 1 2.15 0.99 1 1.00 2.0 3.00 4 ▇▇▁▇▂ schtype 0 1 0.05 0.21 0 0.00 0.0 0.00 1 ▇▁▁▁▁ schurban 0 1 0.71 0.45 0 0.00 1.0 1.00 1 ▃▁▁▁▇ schdenom 0 1 0.16 0.36 0 0.00 0.0 0.00 1 ▇▁▁▁▂ Read in the scottish school leaver data (this file) Use ggplot to visualise how much variation in score there is within and between schools. You might need to subset the data to avoid being overwhelmed. Hints One option would be a boxplot where schoolid is the x axis. Another would be to use stat_summary which would show the mean and SE for each school. Use group_by and summarize to calculate the mean and variance of score within each of the schools, and the sample size in each school (you can use n = length(score) to count the number of observations within each school). Plot the mean of each school (y axis) against the \\(1/var\\) of the school (x axis). Think about which observations you ‘trust’ most? Why? What is 1/var? Taking \\(1/var\\) is often called the ‘inverse variance’ of a measurement. The larger \\(1/var\\) is, the more confidence we have in the measurement. If we want to take the average of all schools, then we might ‘weight’ measurements by their precision. This is, in fact, how meta analysis works: smaller studies are down-weighted in proportion to their precision, not just their sample size. The weighted average in a meta analysis could be calculated as where \\(y_i\\) is the outcome in a particular study, \\(i\\). \\(\\hat{y} = \\frac{\\sum_i y_i / \\sigma_i^2}{\\sum_i 1/\\sigma_i^2}\\) What pattern should I notice?! If you make a plot as suggested (below) you can see that, looking at the high precision schools (those on the right of the plot), there are more above the mean than below (dotted line). These are still smaller-school (or, fewer students sampled in these schools) but if we ‘trust’ lower-variance estimates more, then we might want to allow for that when we estimate the mean of all schools. schools.agg &lt;- schools %&gt;% group_by(schoolid) %&gt;% summarize(score.m=mean(score), var=var(score), n=length(score)) schools.agg %&gt;% ggplot(aes(1/var, score.m, size=n)) + geom_point(alpha=.25) + geom_hline(yintercept=mean(schools$score), linetype=&quot;dashed&quot;) &gt; Warning: Removed 8 rows containing missing values (geom_point). Precision-weighting (and mixed models) work on the principle that we want to use information about how precisely each schools’ mean is estimated to inform our overall estimate. Variance-weighting using mixed models Mixed models are another way to include information about variability when estimating the overall mean. Before we do this, let’s calculate the mean of score in the sample as a reference to compare against: mean(schools$score) &gt; [1] 31.09462 We could equally have calculated the mean of all schools using lm: (m.lm &lt;- lm(score ~ 1, data=schools)) &gt; &gt; Call: &gt; lm(formula = score ~ 1, data = schools) &gt; &gt; Coefficients: &gt; (Intercept) &gt; 31.09 In this model the formula is score ~ 1. The 1 represents an intercept, so the model is estimating the average value across all schools with no other predictors. This is not a weighted average though: we are ignoring the information about which school an observation came from. To calculate something more like a precision-weighted estimate—that is, one which takes into account how much variability there was within each school to calculate the average of all schools — we can use a mixed model. In R we use the lmer function (in place of lm): library(lmerTest) (m.lmer &lt;- lmer( score ~ 1 + (1|schoolid), data = schools)) &gt; Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] &gt; Formula: score ~ 1 + (1 | schoolid) &gt; Data: schools &gt; REML criterion at convergence: 286539.2 &gt; Random effects: &gt; Groups Name Std.Dev. &gt; schoolid (Intercept) 7.821 &gt; Residual 16.073 &gt; Number of obs: 33988, groups: schoolid, 508 &gt; Fixed Effects: &gt; (Intercept) &gt; 30.6 The output here has extra information compared to lm. However like lm the coefficients from the model are listed, which lmer calls ‘fixed effects’. In this case we only have an intercept, which we can compare with the lm output. What is the difference between the weighted and unweighted average for all schools to 2 d.p.? Show answer The difference is 0.49. One way to think about what is happening here is to compare the average for individual schools with the adjusted predictions made by the model. The function ranef(m.lmer) gives us a list of the estimated (weighted) averages for each school from the lmer model. We can make a plot which shows how much the weighted averages ‘shrink’ towards the overall mean. In the plot below: the blue dashes are the adjusted estimates for each of 100 schools the grey bubbles are the unweighted averages of those schools the size of the grey bubbles represents the number of students sampled at that school the blue and red lines are the weighted/unweighted average of all schools Examine the plot above. Which schools ‘shrink’ most? That is, which have the biggest difference between unweighted and weighted values? Does the pattern make sense to you? How do you feel about the size of the change (i.e. difference between red and blue lines)? Should we care? Show discussion The schools that are i) furthest from the mean and/or ii) smallest in size shrink most. This makes sense because these are the schools that we are least confident about. The red line might not seem that different, but remember that the range of the y axis has to be large to show all the inter-school variation. A change in 0.5 points on this scale might well be important, and in limited-power settings (i.e. small psychological experiments) could also result in a change to the inference we make. ‘Random’ intercepts In the model above, we told lmer that schoolid was a grouping variable in our data. The formula was: y ~ 1 + (1|grouping) You can see from this formula there are actually two types of intercept in the model. The first 1 indicates we should fit the intercept representing the mean of all schools The second 1, in the (1|grouping) part of the formula tells lmer to fit a random intercept. By random intercept, we actually mean fit an intercept (mean) for each school individually. This means the model is like the one shown in the lecture slides: The coloured lines in this plot are the random intercepts, and the dotted line is the overall intercept (average of all schools). Look at the area covered by the grey squares, and the coloured squares, in the plot below. Can you identify which numbers in the lmer output would refer to the size of each set of squares? Grey squares: Coloured squares: Show hint The grey squares relate to the schoolid term in the ‘Random effects’ section. The coloured squares are the Residual. Show answers grp vcov squares schoolid 61.17 Grey Residual 258.4 Coloured Summary A simple average ignores how much variation there was within schools. The inverse of the variance is a measure of precision. Adjusting our estimate for sample size and precision can change our estimate of the overall mean Uncertain values are ‘shrunk’ towards the overall mean. That is: if a school has lots of variability, and a mean which is far from the overall average, we shrink our estimate for that school to account for the uncertainty. To do this in R: use lmer not lm include the grouping variable at the end of the formula (i.e. (1|grouping)) the lmer model splits the total variance within and between schools (as we did in the example plots) Can you think of examples in experimental settings where ‘shrinkage’ might be useful? Discuss why this might be the case. Show an example One example would be in experiments where participants make many responses, e.g. in a computer-based task. If a participant is very variable in their responding then we might be less-certain about our estimate of their mean response. Traditionally experimenters might deal with variable responding by excluding participants or data. But this can be arbitrary (where do we set the threshold?) and also doesn’t deal fully with the imprecision. For example, we might exclude responses were an RT is &gt; 3000ms, but this would still allow some participants to be much more variable than others. ‘Shrinking’ variable-responders towards the overall mean is a more principled way of dealing with this lack of precision. It avoids arbitrary cutoffs, can and might increase power when some participants have responded eratically or didn’t fully engage with the task. It also enables us to explore variability within and between participants directly, as we’ll see later. Variance components We will cover this in more detail later, but it’s worth also looking at another part of the lmer output. This time we pass the saved model to the summary function: m.lmer %&gt;% summary() &gt; Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ &gt; lmerModLmerTest] &gt; Formula: score ~ 1 + (1 | schoolid) &gt; Data: schools &gt; &gt; REML criterion at convergence: 286539.2 &gt; &gt; Scaled residuals: &gt; Min 1Q Median 3Q Max &gt; -2.9764 -0.7011 0.1017 0.7391 3.0819 &gt; &gt; Random effects: &gt; Groups Name Variance Std.Dev. &gt; schoolid (Intercept) 61.17 7.821 &gt; Residual 258.36 16.073 &gt; Number of obs: 33988, groups: schoolid, 508 &gt; &gt; Fixed effects: &gt; Estimate Std. Error df t value Pr(&gt;|t|) &gt; (Intercept) 30.6006 0.3698 450.7146 82.74 &lt;2e-16 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the output above we can see a section called ‘Random effects’. By ‘random’, lmer is referring to estimates of variance not that the results are random! As an aside, the ‘mixed’ in the name mixed models refers to the mix of ‘fixed’ and ‘random’ effects. We’ll discuss these terms more later on. We can see the there are two lines in this table, marked: schoolid (intercept) Residual If we add the Variance values together we get 320. This is, the total variance in the score outcome. However, because we have the variance split into two numbers we can calculate the proportion of total variance explained by differences between schools (what is left over is explained by variation between children). \\(var_{j} = \\frac{between}{total}\\) Run summary on the saved lmer model and calculate the proportion of variance which is between schools, and how much is within schools. Show answer 61.2 / 319.6 * 100 = 19% "],
["mixed-models.html", "Mixed models", " Mixed models In brief. In this session we introduce model formulas for mixed models and specify random intercepts and random slopes models. We also introduce the likelihood ratio test, and significance tests for random effects parameters. Slides for today are here: mixed-models.pptx "],
["piglets-random-slope.html", "Piglets (random intercepts)", " Piglets (random intercepts) You can load the piglet data from this URL: pigs &lt;- read_csv(&#39;http://bit.ly/psystats-pigs&#39;) pigs %&gt;% glimpse ## Observations: 432 ## Variables: 3 ## $ id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,… ## $ week &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3,… ## $ weight &lt;dbl&gt; 24.0, 32.0, 39.0, 42.5, 48.0, 54.5, 61.0, 65.0, 72.0, 22.5, 30… The data are from a study of piglet weights, in the 9 weeks after they are weaned. There are multiple piglets (identified by the id variable). Weights are in kg. Load the lmerTest package, and tidyverse, before you begin. How? library(lmerTest) library(tidyverse) Draw a smoothed line plot showing the average increase in weight over the weeks. Show me what that might look like Note that I set geom_point(alpha=.2) to reduce the weight of the points plotted, and make the blue smoothed line more prominent. Fit a random intercepts model where piglet weights are predicted by week. Remind me how pigs.ri &lt;- lmer(weight ~ week + (1|id), data=pigs) summary(pigs.ri) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: weight ~ week + (1 | id) ## Data: pigs ## ## REML criterion at convergence: 2033.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.7390 -0.5456 0.0184 0.5122 3.9313 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 15.142 3.891 ## Residual 4.395 2.096 ## Number of obs: 432, groups: id, 48 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 19.35561 0.60314 58.55889 32.09 &lt;2e-16 *** ## week 6.20990 0.03906 383.00000 158.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## week -0.324 Based on the model, what weight do the piglets start at? Based on the model, how fast are the piglets growing? Show answers The coefficients table has the information we need: pigs.ri %&gt;% fixef() ## (Intercept) week ## 19.355613 6.209896 The pigs start at an average of 19.36 + pigres[2]=sum(pigres)`kg, and increase by 6.21 each week. This corresponds fairly closely to averages calculated from the raw data: pigs %&gt;% filter(week %in% c(1,9)) %&gt;% group_by(week) %&gt;% summarise(mean(weight)) %&gt;% pander() week mean(weight) 1 25.02 9 75.22 "],
["r2-for-mixed-models.html", "\\(R^2\\) for mixed models", " \\(R^2\\) for mixed models In the session I mentioned \\(R^2\\) for mixed models. To calculate this you need to use the MuMIn::r.squaredGLMM function. The literature underpinning the calculations is fairly complex (see Nakagawa and Schielzeth 2013, and @johnson2014extension) but the key idea is that we now have two different \\(R^2\\) values: The marginal \\(R^2\\), which is the variance explained by the fixed effects (this is most similar to conventional \\(R^2\\) from a linear model) The conditional \\(R^2\\), which is the variance explained by both the fixed effects, and the random effects. Using the random intercepts model you ran in the previous section (where piglet weights were predicted by time), use the MuMIn::r.squaredGLMM function to calculate the marginal and conditional \\(R^2\\). How much variance is explained by the week variable? How much extra variance can we explain by including the random intercepts? Show me the code library(MuMIn) pigs.ri &lt;- lmer(weight ~ week + (1|id), data=pigs) r.squaredGLMM(pigs.ri) %&gt;% round(2) %&gt;% pander() R2m R2c 0.93 0.98 R2m refers to the marginal \\(R^2\\), R2c refers to the conditional value. This means that, without the random intercepts, we could explain about 93% of the variance. With random intercepts we can explain over 98%. References "],
["random-slopes.html", "Random slopes", " Random slopes In the session we discussed how to add additional ‘random slope’ terms. That is, terms which allow the effect of predictors to vary between individuals (or other groupings in the data). We did this by adding variables within the parentheses which describe the ‘random’ part of the model. For example, a formula might be: outcome ~ predictor + (predictor | grouping) Here we allow the slope for predictor to vary for different levels of grouping. More concretely, we might have formulae like this, which allows the income of different individuals to grow at different rates over time: income ~ time + (time | person) Or this one, where the effect of intervention are allowed to vary between different schools: reading_age ~ intervention + (intervention | school) Fit a new model, which allows the piglets to grow at different rates over the weeks of the study. Show me how # here pigs.rs stands for &#39;random slopes&#39; pigs.rs &lt;- lmer(weight ~ week + (week|id), data=pigs) Note: if you get a warning about ‘model failed to converge’ don’t worry for now. This IS important, but we will discuss in the next session. Compare the average slope for week between this random slopes model, and the random intercepts model you ran before. Show me how pigs.ri %&gt;% fixef %&gt;% round(2) ## (Intercept) week ## 19.36 6.21 pigs.rs %&gt;% fixef %&gt;% round(2) ## (Intercept) week ## 19.36 6.21 This code shows only the regression parameters (the fixed effects) for the two models. There’s no difference between them in this case (although that wouldn’t always be true). That doesn’t mean that including random slopes isn’t worthwhile though — even though the slope doesn’t change, the confidence interval for it does (it is a little bit wider): pigs.ri %&gt;% confint() ## 2.5 % 97.5 % ## .sig01 3.167027 4.796661 ## .sigma 1.953849 2.250896 ## (Intercept) 18.165548 20.545679 ## week 6.133241 6.286550 pigs.rs %&gt;% confint() ## 2.5 % 97.5 % ## .sig01 2.1026641 3.3056531 ## .sig02 -0.3559056 0.2459552 ## .sig03 0.4969054 0.7646026 ## .sigma 1.1738196 1.3655700 ## (Intercept) 18.5564017 20.1548259 ## week 6.0277615 6.3920302 In this case the effect is so large it’s not important, but in many real datasets it will be. "],
["likelihood-ratio-tests.html", "Likelihood ratio tests", " Likelihood ratio tests Wikipedia helpfully defines the likelihood ratio test for us: In statistics, a likelihood ratio test (LR test) is a statistical test used for comparing the goodness of fit of two statistical models — a null model against an alternative model. The test is based on the likelihood ratio, which expresses how many times more likely the data are under one model than the other. This likelihood ratio … can then be used to compute a p-value … to decide whether to reject the null model. In R, the anova function can be used to compare two models of the same type. We just write: anova(model1, model2) For standard regression, this will produce an F test For mixed models (if you have loaded the lmerTest package), the anova function will actually returns a \\(\\chi^2\\) statistic from the likelihood ratio test. In both cases the interpretation is the same: The test says whether the extra predictors in the more complex model explain a significant amount of extra variation. A significant p value (e.g. &lt; .01) tells us than the more complex model provides a significantly better fit to the data (actually, that the simpler model is significantly worse). A non-significant p value tells us that the extra complexity is not worth the bother and we should prefer the simpler model. The big caveat here is that p values tell you that you can’t reject the null, which isn’t the same thing as rejecting an experimental hypothesis. Use Bayes factors to express evidence for or against a hypothesis of interest. Use the anova command to compare the random intercept and random slope models. Show the code anova(pigs.ri, pigs.rs) ## Data: pigs ## Models: ## pigs.ri: weight ~ week + (1 | id) ## pigs.rs: weight ~ week + (week | id) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## pigs.ri 4 2037.8 2054.1 -1014.93 2029.8 ## pigs.rs 6 1749.9 1774.3 -868.96 1737.9 291.93 2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How do you interpret this output? Does the more complex random slope model provide a significantly better fit? Check answer Yes - it does. The p value is very small, and may be showin in scientific notation (e.g. &lt;2e-16). The likelihood ratio test is telling us that the random slopes model explains significantly more variation in the outcome. Remember though: Just because this is statistically significant it does not mean it’s of any practical importance. Whether that is very meaningful will depend on the context, the sample size and the size of the effect. Reporting a LR test in prose To report the test above you could write something like: The random intercept model was significantly improved by adding a random slope for weeks, χ2(2) = 291.93, p &lt; .001. Or, if you plan to share your models code in a data supplement, you migth like to focus more on the interpretation rather than emphasising how the test was calculated and say: There was substantial variation in growth rates between-piglets, χ2(2) = 291.93, p &lt; .001. At this point you would probably also want to use \\(R^2\\) to quantify how much variation is being explained in each case (the conditional \\(R^2\\)). A shortcut for testing random effects As mentioned in class, the ranova function is a shortcut for testing random effects in an lmer model. To test if the random intercepts explain additional variance run: ranova(pigs.ri) ## ANOVA-like table for random-effects: Single term deletions ## ## Model: ## weight ~ week + (1 | id) ## npar logLik AIC LRT Df Pr(&gt;Chisq) ## &lt;none&gt; 4 -1016.9 2041.8 ## (1 | id) 3 -1253.5 2512.9 473.15 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This shows the comparison of the random intercepts model against a simpler model which ignores clustering by-piglet. We can clearly see that piglets do differ from one another (the p value is very small). This is the same test for the random slopes: ranova(pigs.rs) ## ANOVA-like table for random-effects: Single term deletions ## ## Model: ## weight ~ week + (week | id) ## npar logLik AIC LRT Df Pr(&gt;Chisq) ## &lt;none&gt; 6 -870.44 1752.9 ## week in (week | id) 4 -1016.90 2041.8 292.93 2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You will see a column in the output called LRT and a corresponding p value for the \\(\\chi^2\\) test. You also see AIC values, which are an alternative way of comparing the models popular in some disciplines. See this stackoverflow post ((https://stats.stackexchange.com/users/72352/corey979), n.d.) for a nice summary of Burnham and Anderson (2004). A difference in AIC values between models &gt; 3 or 4 is a good sign that the more complex model is justified. References "],
["school-leavers-longer-exercise.html", "Scottish school leavers data", " Scottish school leavers data Combine the techniques you have used above to re-analyse the Scottish school leavers dataset we saw in session 1. In the data there is a variable called cohort90. This represents which year the data were sampled in relative to 1990: a value of -6 means the data were sampled in 1984. Read in the data from (this file) Plot a graph to explore whether there have been changes in score across cohorts Plot a graph to show change for 20 individual schools (remember that you need to group and summarise the data first to get the average for each school in each cohort; see hint below for some tricks we have not covered before). What pattern do you see in the plot? Fit a model including random intercepts for schools, and a fixed effect (slope) for cohort Fit another model, allowing for variation in the rate of change between schools (random slopes model) Is there significant variation in the rate of change between schools? How do you know? How much variance is explained by the preferred model? Show hint for one of the plots above To make the second plot requested above you need to pre-process the data. In the code below I: Create a list of schools we want to sample using the sample and unique functions Filter the dataset, using the %in% operator (there are other ways, but this is a neat one) Group by school and cohort to then Summarise (calculate the mean) schools &lt;- read_csv(&#39;data/cmm/5.1.txt&#39;) schools_to_sample &lt;- sample(schools$schoolid %&gt;% unique, 20) schools %&gt;% filter(schoolid %in% schools_to_sample) %&gt;% group_by(schoolid, cohort90) %&gt;% summarise(score=mean(score)) ## # A tibble: 113 x 3 ## # Groups: schoolid [20] ## schoolid cohort90 score ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 40 -6 40.5 ## 2 40 -4 30.2 ## 3 40 -2 28.4 ## 4 40 0 39.4 ## 5 40 6 35 ## 6 40 8 39.4 ## 7 51 -6 4.5 ## 8 51 -4 16.8 ## 9 51 -2 16 ## 10 51 0 26.2 ## # … with 103 more rows "],
["random-effects.html", "Random effects", " Random effects In brief In this session we specify more complex random effects, including examples from experimental data with multiple sources of variation. We use visualisation techniques to explore the concept of ‘shrinkage’. Find the slides from the session here "],
["sleep-centering.html", "Sleep (centering)", " Sleep (centering) In the previous session we saw how random intercepts can be used to fit different slopes for different individuals (or groupings) in the data. The following data show the increased in reaction times over the course of a 10 days experiment in which participants suffered sleep deprivation: Is there substantial variation between subjects in reaction times on average? Is there substantial variation in the effect of sleep deprivation? Solution There’s certainly variation in the slopes between individuals, and the effect of sleep deprivation does seem to differ markedly between individuals. However, deciding whether there is variation between people on average is less clear-cut. We need to decide on which day we want to compare the variation. For example, at day 0 there doesn’t seem to be much variation. But by day 5 there does, and by day 10 there is a lot. As we’ll see, to estimate this variation from a model we need to be careful about how we include our covariates (e.g. days in this case). Centering coefficients The plot above was based on the following random slopes model: (sleep.m1 &lt;- lmer(Reaction ~ Days + (Days | Subject), ss)) Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] Formula: Reaction ~ Days + (Days | Subject) Data: ss REML criterion at convergence: 1743.628 Random effects: Groups Name Std.Dev. Corr Subject (Intercept) 24.737 Days 5.923 0.07 Residual 25.592 Number of obs: 180, groups: Subject, 18 Fixed Effects: (Intercept) Days 251.41 10.47 If we look at the dataset, we can see that there are 10 rows for each Subject, with days indexed from 0 to 9. sleepstudy %&gt;% head(15) %&gt;% pander() Reaction Days Subject 249.6 0 308 258.7 1 308 250.8 2 308 321.4 3 308 356.9 4 308 414.7 5 308 382.2 6 308 290.1 7 308 430.6 8 308 466.4 9 308 222.7 0 309 205.3 1 309 203 2 309 204.7 3 309 207.7 4 309 As we know from linear regression, when we include parameters thet estimated in such a way that when we look at them individually they are the estimate for an individual row if all the other predictors were set to zero. In multiple regression this is important because it affects how we interpret the paramaters. When we include multiple predictors it can be hard to read the table of model parameters because we need to add together multiple paramaters to make a prediction. We can get around this using the predict function, but the key is that the scale of the For example, if we use the mtcars data we can see that to estimate a heavy automatic car (high wt and am=1), we would have to do some arithmetic: lm(mpg~wt*am, data=mtcars) Call: lm(formula = mpg ~ wt * am, data = mtcars) Coefficients: (Intercept) wt am wt:am 31.416 -3.786 14.878 -5.298 Because weight in this sample ranges from 1.513 to 5.424, simply adding the intercpt, wt and am paramaters would not be an estimate for any car in the sample, because none of them had zero weight. If, on the other hand, we had mean-centered wt — that is, subtracted the mean from each value — then we could interpret the other coefficients as being the prediction when wt was at the average value in the sample. mt2 &lt;- mtcars %&gt;% mutate(wt = wt - mean(wt)) lm(mpg~wt*am, data=mt2) Call: lm(formula = mpg ~ wt * am, data = mt2) Coefficients: (Intercept) wt am wt:am 19.236 -3.786 -2.168 -5.298 By centering the wt predictor the other coefficients change (although the model itself is really the same—the amount of variance explained is identical, for instance): Show R squared figures # original broom::glance(lm(mpg~wt*am, data=mtcars)) %&gt;% pull(r.squared) [1] 0.8330375 # centered broom::glance(lm(mpg~wt*am, data=mt2)) %&gt;% pull(r.squared) [1] 0.8330375 Why centering matters especially for mixed models In multiple regression centering predictors can be a convenience, but doesn’t affect the model fit1. We can always do arithmetic on our coefficients to convert from one configuration to the another when making predictions. This is not true for mixed models. How we include predictors changes estimates for our variance paramaters. To see why, consider the example we began with, from the sleepstudy data: If we centered the Days coefficient, the plot would look like this: Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] Formula: Reaction ~ Days + (Days | Subject) Data: ss REML criterion at convergence: 1743.628 Random effects: Groups Name Std.Dev. Corr Subject (Intercept) 24.737 Days 5.923 0.07 Residual 25.592 Number of obs: 180, groups: Subject, 18 Fixed Effects: (Intercept) Days 251.41 10.47 Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] Formula: Reaction ~ Days.c + (Days.c | Subject) Data: ss REML criterion at convergence: 1743.628 Random effects: Groups Name Std.Dev. Corr Subject (Intercept) 37.522 Days.c 5.921 0.75 Residual 25.593 Number of obs: 180, groups: Subject, 18 Fixed Effects: (Intercept) Days.c 298.51 10.47 Nothing has changed, except the scale of the x axis. The x axis is now Days.c which ranges from -5 to 5, and 0 is in the middle; previously Days ranged from 0 to 9, with 5 as the midpoint. BUT this small change is important because our paramaters are estimates when all the other predictors = zero. This means that when we use the centered predictor our random intercept variance — the variation between people — is estimated for the midpoint of the study, because day 5 is coded as zero in Days.c. We can see the effect on the variance paramaters if we compare the output of VarCorr. First uncentered: VarCorr(sleep.m1) Groups Name Std.Dev. Corr Subject (Intercept) 24.7366 Days 5.9229 0.066 Residual 25.5918 And then centered: VarCorr(sleep.m1.c) Groups Name Std.Dev. Corr Subject (Intercept) 37.5221 Days.c 5.9212 0.753 Residual 25.5930 When we centre Days then the proportion of variance attributed to subjects changes from about 80% to over 86%. This plot simplifies and shows the difference between models with centered and uncentered predictors. The vertical dashed lines show where the variance for the random intercept would be taken from in each case: What’s the right thing to do? It depends on what your question is! In the sleepstudy example we might argue that the first (uncentered) model makes more sense because there is a definite start to the experiment: At Day 0 we have people in their ‘natural’ state, before we deprive them of sleep. So in this model the (Intercept) term describes the varation between people in the normal state. However in other cases this wouldn’t be true. For example, let’s say we used the Scottish schools data and included the year each cohort was taken in as a predictor: schools.1 &lt;- lmer(score ~ year + (year|schoolid), data=schools) VarCorr(schools.1) Groups Name Std.Dev. Corr schoolid (Intercept) 14.770619 year 0.011085 -1.000 Residual 14.796990 Now, the between-school variance is estimated in the year 0 (i.e. &gt;2000 years ago), which clearly doesn’t make much sense. If we compare the ICC (or % variance-attributable) within and between schools it makes a huge difference: schools.2 &lt;- lmer(score ~ cohort90 + (cohort90|schoolid), data=schools) bind_rows( VarCorr(schools.1) %&gt;% as.tibble() %&gt;% mutate(Centered=&quot;No&quot;), VarCorr(schools.2) %&gt;% as.tibble() %&gt;% mutate(Centered=&quot;Yes&quot;)) %&gt;% filter(is.na(var2)) %&gt;% group_by(Centered) %&gt;% mutate(icc=round(vcov / sum(vcov) * 100, 2)) %&gt;% select(grp, var1, icc) %&gt;% rename(Grouping=grp, Parameter=var1, `%`=icc) %&gt;% pander() Centered Grouping Parameter % No schoolid (Intercept) 49.91 No schoolid year 0 No Residual NA 50.09 Yes schoolid (Intercept) 16.6 Yes schoolid cohort90 0.06 Yes Residual NA 83.34 In this case it makes no sense to estimate the variance when year = 0 and we should definitely center the variables. If you’re not sure, however, it’s probably safer to center predictors in mixed models. Return to the piglets example from sessions 1 and 2. Refit the model with centered predictors Calculate the variance partition by differences between piglets using centered and uncentered models. What differences do you spot? Which parameterisation most sense in this case? Are we more interested in variation between pigs at the start or the middle of the study? This is mostly true. The only exception is when parameters are collinear (see PSYC753 materials); in this case numerical issues when fitting models can lead to small differences, but this is mostly not the case.↩︎ "],
["mma.html", "MMA", " MMA photo:pixabay The following data are a tidied-up version of the data made available by the authors of the MMA paper we discussed in class (Pavelka et al. 2020). The included column tells whether the row was included in the authors’ analysis from the paper. By filtering out non-included rows we can make our results match exactly. mma &lt;- read_csv(&#39;data/mma_bw.csv&#39;) %&gt;% filter(included==TRUE) Import the tidied the data (from this file) Fit the model described in the paper (in their supplement the authors reveal that stimulus and FI were mean-centered). How do I mean-centre things? mma.c &lt;- mma %&gt;% mutate(FI.c = FI - mean(FI), stimulus.c = stimulus - mean(stimulus)) mma.c %&gt;% head # A tibble: 6 x 8 person condition RT FI stimulus included FI.c stimulus.c &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 before 251 57.2 6 TRUE 13.5 -10.5 2 1 before 264 57.2 7 TRUE 13.5 -9.51 3 1 before 221 57.2 8 TRUE 13.5 -8.51 4 1 before 221 57.2 9 TRUE 13.5 -7.51 5 1 before 256 57.2 10 TRUE 13.5 -6.51 6 1 before 281 57.2 11 TRUE 13.5 -5.51 Show the model (mma.m1 &lt;- lmer(RT ~ 1 + condition + stimulus.c + FI.c + condition:stimulus.c + (1|person), data=mma.c)) Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] Formula: RT ~ 1 + condition + stimulus.c + FI.c + condition:stimulus.c + (1 | person) Data: mma.c REML criterion at convergence: 22125.95 Random effects: Groups Name Std.Dev. person (Intercept) 38.46 Residual 40.02 Number of obs: 2150, groups: person, 45 Fixed Effects: (Intercept) conditionbefore 270.18446 -4.03080 stimulus.c FI.c -0.26609 -0.25665 conditionbefore:stimulus.c -0.05753 Look at the effect of condition in your model. Does this match the authors’ report that “POST RTs were found significantly higher than PRE (mean difference = 4.031 ms, SE = 1.726”? Show answer Yes: the conditionbefore coefficient from your model should have been -4.03 also. The direction is reversed because the authors’ encoded their model slightly differently (their parameter would have been called conditionafter) but the meaning is the same. To see the SE, use summary and coef together: summary(mma.m1) %&gt;% coef() %&gt;% pander() Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 270.2 5.861 44.93 46.1 1.777e-39 conditionbefore -4.031 1.726 2102 -2.335 0.01964 stimulus.c -0.2661 0.1766 2102 -1.507 0.132 FI.c -0.2566 0.6756 43 -0.3799 0.7059 conditionbefore:stimulus.c -0.05753 0.2495 2102 -0.2306 0.8176 This is 0.17660, which is what the authors report. Are the reported findings robust to minor changes in the model fitting? For example, if parameters are removed or if the excluded data is incorporated? Show details This model, excluding fatigue (FI), predicts the same 4ms difference: mma.m2 &lt;- lmer(RT ~ 1 + condition * stimulus.c + (stimulus.c|person), data=mma.c) summary(mma.m2) %&gt;% coef() %&gt;% pander() Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 270.2 5.804 45.98 46.55 2.49e-40 conditionbefore -4.027 1.712 2058 -2.352 0.01876 stimulus.c -0.2673 0.2072 106.4 -1.29 0.1999 conditionbefore:stimulus.c -0.05712 0.2474 2059 -0.2309 0.8174 However this model fit includes all the excluded data and predicts a 2ms difference in the other direction, and is not statistically significant: mma.all &lt;- read_csv(&#39;data/mma_bw.csv&#39;) %&gt;% mutate(stimulus.c = stimulus-mean(stimulus), FI.c = FI-mean(FI), ) mma.m3 &lt;- lmer(RT ~ 1 + condition + stimulus.c + FI.c + condition:stimulus.c + (1 | person), data=mma.all) mma.m3 %&gt;% summary() %&gt;% coef() %&gt;% pander() Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 271 6.129 51.92 44.22 6.936e-43 conditionbefore 2.198 3.685 2467 0.5964 0.5509 stimulus.c -0.3548 0.3225 2467 -1.1 0.2715 FI.c 0.3986 0.6813 42.99 0.585 0.5616 conditionbefore:stimulus.c -1.169 0.4565 2467 -2.561 0.01051 How should we interpret these findings, based on the replications you have made? References "],
["politeness.html", "Politeness", " Politeness Winter &amp; Grawunder, 2012 describe a study of the pitch (frequency) of individuals’ speech when they recorded different phrases (scenarios). The scenarios differed in whether they required ‘politeness’ or were more informal in nature. A reduced version of this dataset is also described and analysed in a mixed-models tutorial (see http://www.bodowinter.com/uploads/1/2/9/3/129362560/bw_lme_tutorial2.pdf). The data are available online and can be read from this URL: # this previously taken from http://www.bodowinter.com/tutorial/politeness_data.csv but that link is now broken politeness &lt;- read_csv(&quot;https://raw.githubusercontent.com/opetchey/BIO144/master/3_datasets/politeness_data.csv&quot;) Make a plot showing how average levels of frequency differs between individuals. Show me the plot politeness %&gt;% ggplot(aes(subject, frequency)) + geom_boxplot() Make a plot showing how frequency differs between scenarios Show me the plot politeness %&gt;% ggplot(aes(factor(scenario), frequency)) + geom_boxplot() Based on the plots, which do you think accounts for more variation in frequency: subject or scenario? Show answer It looks like subjects vary more in frequency than do scenarios, but there appears to be variation attributable to both variables. Fit a random intercepts model to the data, allowing for variation between subjects. Include fixed effects for attitude and gender: Show me that model (polite.ri.subject &lt;- lmer(frequency ~ attitude + gender + (1|subject), data=politeness)) Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] Formula: frequency ~ attitude + gender + (1 | subject) Data: politeness REML criterion at convergence: 786.7439 Random effects: Groups Name Std.Dev. subject (Intercept) 24.57 Residual 29.17 Number of obs: 83, groups: subject, 6 Fixed Effects: (Intercept) attitudepol genderM 256.69 -19.41 -108.20 What other commands should I run to interpret the model at this point? You might then also want to look at the regression coefficients, Anova table, and tests of random effects: # regression coefficients polite.ri.subject %&gt;% summary %&gt;% coef() %&gt;% pander(caption=&quot;Model coefficients&quot;) Model coefficients Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 256.7 15.23 4.378 16.86 3.78e-05 attitudepol -19.41 6.407 76.02 -3.03 0.003345 genderM -108.2 21.06 4.009 -5.137 0.006766 # anova-table anova(polite.ri.subject) %&gt;% pander(caption=&quot;Anova table&quot;) Anova table Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) attitude 7810 7810 1 76.02 9.179 0.003345 gender 22457 22457 1 4.009 26.39 0.006766 # tests of random effects ranova(polite.ri.subject) %&gt;% pander(caption=&quot;Test of random intercepts&quot;) Test of random intercepts npar logLik AIC LRT Df Pr(&gt;Chisq) 5 -393.4 796.7 NA NA NA (1 | subject) 4 -404.7 817.4 22.68 1 1.909e-06 Fit a second random intercepts model, allowing for variation in both subjects and between the different scenarios. Show me that model polite.ri.both &lt;- lmer(frequency ~ attitude + gender + (1|subject) + (1|scenario), data=politeness) # test of random effect ranova(polite.ri.both) ANOVA-like table for random-effects: Single term deletions Model: frequency ~ attitude + gender + (1 | subject) + (1 | scenario) npar logLik AIC LRT Df Pr(&gt;Chisq) &lt;none&gt; 6 -387.73 787.45 (1 | subject) 5 -402.50 815.00 29.546 1 5.461e-08 *** (1 | scenario) 5 -393.37 796.74 11.289 1 0.0007796 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How much variance was attributable to subjects or scenarios, compared with the total variance? Show me how to calculate this Remember to convert the VarCorr output to a dataframe to see the variance (rather than standard deviations) in the vcov column. You can then use mutate or transmute to calculate the within/betwen ratio (the ICC). polite.ri.both %&gt;% VarCorr() %&gt;% as_tibble() %&gt;% transmute( Term=Hmisc::capitalize(grp), `% variation`=round(vcov / sum(vcov) * 100, 1) ) %&gt;% pander() Term % variation Scenario 14.8 Subject 41.6 Residual 43.6 Add random slopes to the model above. Allow the effect of attitude (whether the scenario was polite or informal) to differ between subjects, and also to differ between the different example scenarios sampled in this study. Show me that model formula frequency ~ attitude + gender + (attitude|subject) + (attitude|scenario) frequency ~ attitude + gender + (attitude | subject) + (attitude | scenario) Use ranova to test whether the random slopes improved the model Show me how First run the model: polite.rslopes &lt;- lmer(frequency ~ attitude + gender + (attitude|subject) + (attitude|scenario), data=politeness) If you see a message saying boundary (singular) fit: see ?isSingular don’t worry for now. This IS actually quite important, but we will discuss it in more detail in the next session. And then use ranova: ranova(polite.rslopes) ANOVA-like table for random-effects: Single term deletions Model: frequency ~ attitude + gender + (attitude | subject) + (attitude | scenario) npar logLik AIC LRT Df Pr(&gt;Chisq) &lt;none&gt; 10 -387.54 795.08 attitude in (attitude | subject) 8 -387.55 791.10 0.02468 2 0.9877 attitude in (attitude | scenario) 8 -387.71 791.43 0.35218 2 0.8385 Both of the Pr(&gt;Chisq) values (the p values for the chi squared test) are non-significant. You might also note this more complex model had some problems fitting: Don’t worry for now, but we will cover with this in the final session. If a random slopes model is not ‘significantly’ better than a similar model which does not include the random slope term, is there any reason why we might still prefer it, and use the ‘full’ model to base our inference on? Show answer Yes - simulation studies, including Barr et al 2013 suggest that a ‘maximal’ model is likely to be more conservative than a model which excludes non-significant random effects terms. How might mixed models make the problem of ‘researcher degrees of freedom’ worse? What effect might this have? How can it be avoided? Show answer Mixed models provide many more possible ‘ways to do it’. In addition to different fixed-effects specifications we can now also have many different random effects specifications. This increases degrees of freedom in the analysis, and makes it even more important to pre-specify analyses. "],
["model-complexity.html", "Model complexity", " Model complexity Although Barr et al. (2013) exhort researchers to ‘keep it maximal’, and include as many random effects in the model as implied by the design, this isn’t always possible. Mixed models with many random effects can be difficult to fit. You will already have seen error messages about ‘singular fits’ and ‘non-convergence’. Two important points: Non-converged models, or models with fit errors should NOT be interpreted or reported. Results from them are quite likely to be unstable/incorrect. The easiest/most immediate way to get rid of the errors is to simplify the model Taking one of the examples from above: (sleep.rs &lt;- lmer(Reaction ~ Days + (Days|Subject), data=ss)) Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] Formula: Reaction ~ Days + (Days | Subject) Data: ss REML criterion at convergence: 1743.628 Random effects: Groups Name Std.Dev. Corr Subject (Intercept) 24.737 Days 5.923 0.07 Residual 25.592 Number of obs: 180, groups: Subject, 18 Fixed Effects: (Intercept) Days 251.41 10.47 This model contains 3 random effects parameters which have to be estimated from the data (not including the residual variance): sleep.rs %&gt;% VarCorr Groups Name Std.Dev. Corr Subject (Intercept) 24.7366 Days 5.9229 0.066 Residual 25.5918 They are: Random intercept term for Subject Random slope for Days within Subject The covariance of intercept and slope within Subject Often, we will not have enough data to properly estimate all these paramaters. However, as Barr et al recommend, we can exclude parameters from the model by: Explicitly specifying the random intercept with (1|grouping) Adding another term which is (0 + variable | grouping) Here, the 0+ means remove the random intercept. By specifying the random effects in different parts (i.e. within separate sets of brackets) lmer knows the terms are not correlated. sleep.rs.simplified &lt;- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), data=ss) sleep.rs.simplified %&gt;% VarCorr Groups Name Std.Dev. Subject (Intercept) 25.0499 Subject.1 Days 5.9887 Residual 25.5652 References "],
["extensions.html", "Extensions", " Extensions Factorial anova in mixed models The file journal.pone.0226387.s001.sav contains data from Kemps et al. (2019). These have been reshaped to long format and tidied up in data/drinks.csv. The study explored the effect of an cognitive bias modification intervention, and one of the outcomes was Attentional Bias in a dot-probe task. There were two experimental conditions (between subjects) and participants were tested twice. The original paper reported results from a mixed model in which Time was a repeated/within-subjects factor. drinks %&gt;% head %&gt;% pander(&quot;6 rows from the `drinks` dataset&quot;) 6 rows from the drinks dataset Participant Condition Age Gender Time AttBias 1 Attend 18 Female Pre -5.794 1 Attend 18 Female Post -17.86 2 Avoid 19 Female Pre -5.19 2 Avoid 19 Female Post 26.08 3 Attend 24 Male Pre 27 3 Attend 24 Male Post 15.81 Plotting the data indicates a Time*Condition interaction: Run a random intercepts model which would test the relationships shown in the plot above Solution (drinks.m1 &lt;- lmer(AttBias ~ Condition * Time + (1|Participant), data=drinks)) Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] Formula: AttBias ~ Condition * Time + (1 | Participant) Data: drinks REML criterion at convergence: 2033.767 Random effects: Groups Name Std.Dev. Participant (Intercept) 4.975 Residual 19.592 Number of obs: 232, groups: Participant, 116 Fixed Effects: (Intercept) ConditionAvoid TimePre 9.706 -11.991 -4.684 ConditionAvoid:TimePre 13.086 Use the anova command to test the interaction of Condition and Time Solution drinks.m1 %&gt;% anova Type III Analysis of Variance Table with Satterthwaite&#39;s method Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) Condition 1525.01 1525.01 1 114 3.9729 0.04863 * Time 200.51 200.51 1 114 0.5224 0.47132 Condition:Time 2482.99 2482.99 1 114 6.4685 0.01232 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There does appear to be an interaction. Follow-up tests To run follow-up tests comparing individual cells in the design we can use another R package called emmeans. library(emmeans) The emmeans function (which is inside the emmeans package we just loaded), can calculate means and confidence intervals for each cell in the design: emmeans(drinks.m1, ~Condition*Time) Condition Time emmean SE df lower.CL upper.CL Attend Post 9.71 2.65 227 4.476 14.94 Avoid Post -2.28 2.65 227 -7.515 2.95 Attend Pre 5.02 2.65 227 -0.207 10.25 Avoid Pre 6.12 2.65 227 0.887 11.35 Degrees-of-freedom method: kenward-roger Confidence level used: 0.95 In the code above we used the emmeans function, and gave it the saved random intercept model as input. In the second input (where is says ~Condition*Time) we are using a formula to describe which part of the design we would like means for. Helpfully, you can also plot the results of this function, so the folliwing is a shortcut for writing a ggplot command yourself: plot(emmeans(drinks.m1, ~Condition*Time)) + xlab(&quot;Predicted value&quot;) We can also save the results of emmeans and send it to the contrast function. This produces t tests for each pairwise comparison: savedmeans1 &lt;- emmeans(drinks.m1, ~Condition*Time) contrast(savedmeans1, method=&#39;pairwise&#39;, adjust=&#39;none&#39;) contrast estimate SE df t.ratio p.value Attend,Post - Avoid,Post 11.99 3.75 227 3.195 0.0016 Attend,Post - Attend,Pre 4.68 3.64 114 1.287 0.2006 Attend,Post - Avoid,Pre 3.59 3.75 227 0.956 0.3400 Avoid,Post - Attend,Pre -7.31 3.75 227 -1.947 0.0528 Avoid,Post - Avoid,Pre -8.40 3.64 114 -2.309 0.0227 Attend,Pre - Avoid,Pre -1.09 3.75 227 -0.292 0.7708 Degrees-of-freedom method: kenward-roger And we can add adjust=\"bonferonni\" or adjust=\"tukey\" method to adjust for multiple comparisons contrast(savedmeans1, method=&#39;pairwise&#39;, adjust=&#39;tukey&#39;) contrast estimate SE df t.ratio p.value Attend,Post - Avoid,Post 11.99 3.75 227 3.195 0.0086 Attend,Post - Attend,Pre 4.68 3.64 114 1.287 0.5728 Attend,Post - Avoid,Pre 3.59 3.75 227 0.956 0.7744 Avoid,Post - Attend,Pre -7.31 3.75 227 -1.947 0.2116 Avoid,Post - Avoid,Pre -8.40 3.64 114 -2.309 0.1019 Attend,Pre - Avoid,Pre -1.09 3.75 227 -0.292 0.9913 Degrees-of-freedom method: kenward-roger P value adjustment: tukey method for comparing a family of 4 estimates If we want to compare pairs of groups, we can adjust the formula we pass to the emmeans function. In the example below I compare Conditions averaged across both time periods: savedmeans2 &lt;- emmeans(drinks.m1, ~Condition) contrast(savedmeans2, method=&#39;pairwise&#39;) contrast estimate SE df t.ratio p.value Attend - Avoid 5.45 2.73 114 1.993 0.0486 Results are averaged over the levels of: Time Degrees-of-freedom method: kenward-roger In this example we compare the Pre and Post measurement times within each group (here the bar symbol, |, is read as ‘within’): savedmeans3 &lt;- emmeans(drinks.m1, ~Time|Condition) contrast(savedmeans3, method=&#39;pairwise&#39;) Condition = Attend: contrast estimate SE df t.ratio p.value Post - Pre 4.68 3.64 114 1.287 0.2006 Condition = Avoid: contrast estimate SE df t.ratio p.value Post - Pre -8.40 3.64 114 -2.309 0.0227 Degrees-of-freedom method: kenward-roger Useful emmeans examples # get the means of all combinations of factors A and B emmeans(model, ~A+B) # menas the same as the line above emmeans(model, ~A*B) # show all pairwise contrasts for all combinations of A and B contrast(emmeans(model, pairwise~A+B), pairwise) # show tests of the effect of A within each level of B contrast(emmeans(model, pairwise~A|B), pairwise) # show the confidence interval for the tests above: confint(contrast(emmeans(model, pairwise~A|B), pairwise)) Adjust the code using emmeans and contrast above to compare Pre vs Post scores (averaged across Condition) Compare the conditions within each time period. Solution # compare times averaged across conditions contrast(emmeans(drinks.m1, ~Time), method=&#39;pairwise&#39;) contrast estimate SE df t.ratio p.value Post - Pre -1.86 2.57 114 -0.723 0.4713 Results are averaged over the levels of: Condition Degrees-of-freedom method: kenward-roger # compare conditions within each time contrast(emmeans(drinks.m1, ~Condition|Time), method=&#39;pairwise&#39;) Time = Post: contrast estimate SE df t.ratio p.value Attend - Avoid 11.99 3.75 227 3.195 0.0016 Time = Pre: contrast estimate SE df t.ratio p.value Attend - Avoid -1.09 3.75 227 -0.292 0.7708 Degrees-of-freedom method: kenward-roger References "],
["further-reading.html", "Further reading", " Further reading Meteyard summarises current good practice in running and reporting mixed models (Meteyard and Davies 2020). Barr et al (2013) go into more detail on why mixed models are preferable to RM anova, and this paper will be a useful reference for future sessions too. See also Eager and Roy (2017) which points out some of the failings of mixed models as typically used. References "],
["assessment.html", "Assessment", " Assessment The REFRAMED trial was a large RCT evaluating a variant of dialectical behaviour therapy for patients with treatment-resistant depression (Lynch et al. 2018). The design of the study is described in detail in Lynch et al. (2015). The original, uncentered data, used for this analysis are available here: data/reframed.csv. Make a plot which shows changes in PHQ9 scores over time, split by group. It can either look like the plot below, or display the differences between groups over time in another appropriate way: Changes in PHQ9 Scores during the REFRAMED trial Fit a random intercepts model using lmer, including month as a fixed factor (a categorical variable). Also include treatment group, and the interaction of month and group. Report the model you run and interpret the model output, and report relevant statistics covered during the course. Be sure to report the % variance attributable to between-patient differences. Use the guidelines in Meteyard and Davies (2020) when deciding what to report. Examine the code and output below. The variable phq9.c is a copy of the original dataset in which all the numeric variables have been mean-centered. Give a descriptive label/subtitle to each of the sections A to D below. Comment on each line in the code, explaining what it does, and what the output (if any) means. Use the R help files as a reference to functions if needed. With reference to the plot above, why do you think that observations after month 12 are excluded from this analysis? Given the model formula used and the design of the study, what other observations might it be reasonable to exclude? Which model should we prefer, phq9.m2 or phq9.m3? Is model phq9.m3 ‘maximal’ Barr et al. (2013)? Section A: phq9.c2 &lt;- phq9.c %&gt;% mutate(tx=ifelse(grp==&quot;TAU&quot;, 0, 1)) %&gt;% filter(month &lt; 12) Section B: phq9.m2 &lt;- lmer(phq9 ~ month * tx + (1 | patient), data=phq9.c2) phq9.m3 &lt;- lmer(phq9 ~ month * tx + (1 + month:tx | patient), data=phq9.c2) Section C: ranova(phq9.m2) ANOVA-like table for random-effects: Single term deletions Model: phq9 ~ month + tx + (1 | patient) + month:tx npar logLik AIC LRT Df Pr(&gt;Chisq) &lt;none&gt; 6 -7108.5 14229 (1 | patient) 5 -8104.6 16219 1992.1 1 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ranova(phq9.m3) ANOVA-like table for random-effects: Single term deletions Model: phq9 ~ month + tx + (1 | patient) + (0 + month:tx | patient) + month:tx npar logLik AIC LRT Df Pr(&gt;Chisq) &lt;none&gt; 7 -7050.9 14116 (1 | patient) 6 -8099.0 16210 2096.20 1 &lt; 2.2e-16 month:tx in (0 + month:tx | patient) 6 -7108.5 14229 115.32 1 &lt; 2.2e-16 &lt;none&gt; (1 | patient) *** month:tx in (0 + month:tx | patient) *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Section D: phq9.m2 %&gt;% anova Type III Analysis of Variance Table with Satterthwaite&#39;s method Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) month 752.05 752.05 1 2244.3 49.4829 2.647e-12 *** tx 98.93 98.93 1 250.2 6.5092 0.01133 * month:tx 90.60 90.60 1 2240.6 5.9612 0.01470 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 phq9.m3 %&gt;% anova Type III Analysis of Variance Table with Satterthwaite&#39;s method Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) month 747.70 747.70 1 2102.47 55.4263 1.407e-13 *** tx 93.59 93.59 1 248.58 6.9377 0.008969 ** month:tx 63.91 63.91 1 330.16 4.7373 0.030224 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 References "],
["references.html", "References", " References Barr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013. “Random Effects Structure for Confirmatory Hypothesis Testing: Keep It Maximal.” Journal of Memory and Language 68 (3): 255–78. Burnham, Kenneth P, and David R Anderson. 2004. “Multimodel Inference: Understanding Aic and Bic in Model Selection.” Sociological Methods &amp; Research 33 (2): 261–304. Eager, Christopher, and Joseph Roy. 2017. “Mixed Effects Models Are Sometimes Terrible.” arXiv Preprint arXiv:1701.04858. (https://stats.stackexchange.com/users/72352/corey979). n.d. “How to Compare Models on the Basis of Aic?” Cross Validated. https://stats.stackexchange.com/q/232494. Johnson, Paul CD. 2014. “Extension of Nakagawa &amp; Schielzeth’s R2glmm to Random Slopes Models.” Methods in Ecology and Evolution 5 (9): 944–46. Kemps, Eva, Marika Tiggemann, Mikaela Cibich, and Aleksandra Cabala. 2019. “Cognitive Bias Modification for Energy Drink Cues.” PloS One 14 (12). Lynch, Thomas R, Roelie J Hempel, Ben Whalley, Sarah Byford, Rampaul Chamba, Paul Clarke, Susan Clarke, et al. 2018. “Radically Open Dialectical Behaviour Therapy for Refractory Depression: The Reframed Rct.” Lynch, TR, B Whalley, RJ Hempel, S Byford, P Clarke, S Clarke, D Kingdon, et al. 2015. “Refractory Depression: Mechanisms and Evaluation of Radically Open Dialectical Behaviour Therapy (Ro-Dbt)[REFRAMED]: Protocol for Randomised Trial.” BMJ Open 5 (7). Meteyard, Lotte, and Robert AI Davies. 2020. “Best Practice Guidance for Linear Mixed-Effects Models in Psychological Science.” Journal of Memory and Language 112: 104092. Nakagawa, Shinichi, and Holger Schielzeth. 2013. “A General and Simple Method for Obtaining R2 from Generalized Linear Mixed-Effects Models.” Methods in Ecology and Evolution 4 (2): 133–42. Pavelka, Radim, Vı́t Třebickỳ, Jitka Třebická Fialová, Adam Zdobinskỳ, Klára Coufalová, Jan Havlı́ček, and James J Tufano. 2020. “Acute Fatigue Affects Reaction Times and Reaction Consistency in Mixed Martial Arts Fighters.” PloS One 15 (1): e0227675. "]
]

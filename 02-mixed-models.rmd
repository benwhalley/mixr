---
title: 'Random effects'
author: 'Ben Whalley'
date: "Feb 2021"
bibliography: [references.bib]
biblio-style: apa6
link-citations: yes
output:
  webex::html_clean
---



```{r, include=F, message=F, warning=F}
library(tidyverse)
library(webex)

library(lmerTest)
library(pander)
library(broom)
knitr::opts_chunk$set(cache=T, message=F, warning=F)
```


![](images/randomeffectlines.png)


### In brief.

> In this worksheet we introduce model formulas for mixed models and specify random
> intercepts and random slopes. We also cover the likelihood ratio test, and
> significance tests for random effects parameters.




# Piglets data {#piglets-random-slope}

![](images/piglet-sm.jpg)

```{r, echo=F, eval=F}
# this is where the data came from
#pigs <- readstata13::read.dta13('http://www.stata-press.com/data/r14/pig.dta')
#write_csv(pigs, 'data/pigs.csv')
```


You can load the piglet data you saw in class from this URL:

```{r}
pigs <- read_csv('http://bit.ly/psystats-pigs')
pigs %>% glimpse
```


The data are from a study of piglet weights, in the 9 weeks after they are weaned. There are multiple piglets (identified by the `id` variable). Weights are in kg.


:::{.exercise}


0. Load the `lmerTest` package, and `tidyverse`, before you begin.

`r hide("How?")`

```{r}
library(lmerTest)
library(tidyverse)
```

`r unhide()`


1. Draw a smoothed line plot showing the average increase in weight over the weeks.

`r hide("Show me what that might look like")`

```{r, echo=F}
pigs %>%
  ggplot(aes(week, weight)) +
  geom_point(alpha=.2) +
  geom_smooth(se=F) +
  scale_x_continuous(breaks=1:10)
```

Note that I set `geom_point(alpha=.2)` to reduce the weight of the points plotted, and make the blue smoothed line more prominent.

`r unhide()`


2. Add the argument `group=VAR` to the `aes` part of your plot to show individual lines for each piglet. 
That is, your plot should have as many smoothed lines as there are piglets in the data.

`r hide("Check what that might look like")`

```{r, echo=F}
pigs %>%
  ggplot(aes(week, weight, group=id)) +
  geom_point(alpha=.2) +
  geom_smooth(se=F, size=.2, color="black") +
  scale_x_continuous(breaks=1:10)
```

Note that I set `geom_smooth(size=.2)` to reduce the weight of the lines plotted and  so make individuals easier to distinguish.
I left the  `geom_point` function in place to show both the lines and the original data.

`r unhide()`


3. As an experiment, try colouring each of the lines — that is, give each piglet it's own colour. If you decide you want to 
hide the colour legend of this plot you can add: `+ theme(legend.position="none")` to the end of your code.

`r hide("Check what that might look like")`

```{r, echo=F}
pigs %>%
  ggplot(aes(week, weight, group=id, color=factor(id))) +
  geom_point(alpha=.2) +
  geom_smooth(se=F, size=.2) +
  scale_x_continuous(breaks=1:10) + 
  theme(legend.position="none")
```

Note that there are too many pigs to really distinguish each line properly, so in some ways the use of color 
is inappropriate. However the colours do help a little bit to check that the lines
mostly run parallel and there are no big deviations. This is the sort of plot I might use in 
my own exploration of the data, but not in a publication or report.

`r unhide()`


4. Make a quick note: characterise the growth of the piglets in this dataset. How much variation is there between piglets? Is their growth well described by a linear effect of time?


:::


# Random intercepts models


Random intercept models extend linear regression by splitting the error term, acknowledging grouping/nesting which 
may be present in the sample.



:::{.exercise}


1. Fit a random intercepts model where piglet weights are predicted by `week`.


`r hide("Remind me how")`

```{r}
pigs.ri <- lmer(weight ~ week + (1|id), data=pigs)
summary(pigs.ri)
```


`r unhide()`

---

Use the fixed effects part of the summary output. Remember that fixed effects in these models are just like regression coefficients.

2. What weight do the piglets start at?
3. How fast are the piglets growing?


`r hide("Show answers")`


The coefficients are in the `summary` output, but the `fixef` function can also be used to show only these coefficients:

```{r}
pigs.ri %>% fixef()
```

```{r, echo=F, include=F}
pigres <- pigs.ri %>% fixef %>% round(2)
```

The pigs start at an average of `r pigres[1]` + pigres[2]` = `sum(pigres)`kg, and increase by `r pigres[2]` each week.

This corresponds fairly closely to averages calculated from the raw data:

```{r}
pigs %>% 
  filter(week %in% c(1,9)) %>% 
  group_by(week) %>% 
  summarise(mean(weight)) %>% 
  pander()
```


`r unhide()`


:::









# Random slopes


In the session we discussed how to add additional 'random slope' terms. That is, terms which allow the effect of predictors to vary between individuals (or other groupings in the data).

We did this by adding variables within the parentheses which describe the 'random' part of the model. For example, a formula might be:

```{r, eval=F}
outcome ~ predictor + (predictor | grouping)
```

Here we allow the slope for `predictor` to vary for different levels of `grouping`.


More concretely, we might have formulae like this, which allows the `income` of different individuals to grow at different rates over `time`:

```{r, eval=F}
income ~ time + (time | person)
```


Or this one, where the effect of `intervention` is allowed to vary between different schools:

```{r, eval=F}
reading_age ~ intervention + (intervention | school)
```



:::{.exercise}


1. Fit a new model, which allows the piglets to grow at different rates over the weeks of the study.

`r hide("Show me how")`

```{r}
# here pigs.rs stands for 'random slopes'
pigs.rs <- lmer(weight ~ week + (week|id), data=pigs)
```


Note: if you get a warning about 'model failed to converge' don't worry for now. This IS important, but we will discuss in the next session.


```{r}
# maximal random effects
lmer(weight ~ 1 + (week|id), data=pigs)

# drop covariance for intercept and slope
lmer(weight ~ 1 + (1|id) + (0 + week | id), data=pigs)

# add back fixed effect once random effects sorted
lmer(weight ~ 1 + week + (1|id) + (0 + week | id), data=pigs)

```




`r unhide()`


2. Compare the *average* slope for `week` between this random slopes model, and the random intercepts model you ran before.

`r hide("Show me how")`

```{r}
pigs.ri %>% fixef %>% round(2)
pigs.rs %>% fixef %>% round(2)
```

This code shows only the regression parameters (the fixed effects) for the two models. There's no difference between them in this case (although that wouldn't _always_ be true).

That doesn't mean that including random slopes isn't worthwhile though — even though the slope doesn't change, the confidence interval for it does (it is a little bit wider):


```{r}
pigs.ri %>% confint()
pigs.rs %>% confint()
```

In this case the effect is so large it's not important, but in many real datasets it will be.


`r unhide()`



3. Use the `ranova` function to perform a likelihood ratio on this random effect of time. Is the substantial variance (between piglets) in the rate of growth.

:::






# Variance explained

In the session I mentioned $R^2$ for mixed models. To calculate this you need to use the `r.squaredGLMM` function from the `MuMIn` package..

The literature underpinning the calculations is fairly complex [see @nakagawa2013general and @johnson2014extension] but the key idea is that we now have two different $R^2$ values:

- The *marginal* $R^2$, which is the variance explained by the fixed effects (this is most similar to conventional $R^2$ from a linear model)

- The *conditional* $R^2$, which is the variance explained by both the fixed effects, *and* the random effects.



:::{.exercise}

1. Load the `MuMIn` package.

1. Using the random intercepts model you ran in the previous section  (where piglet weights were predicted by time), use the `r.squaredGLMM` function to calculate the marginal and conditional $R^2$.

2. How much variance is explained by the `week` variable? How much extra variance can we explain by including the random intercepts?

3. Now compare the random intercepts and random slopes models. How much additional variance does the random slope explain?


`r hide("Show me the code")`

```{r}
library(MuMIn)
pigs.ri <- lmer(weight ~ week + (1|id), data=pigs)
r.squaredGLMM(pigs.ri) %>% round(2) %>%  pander()

pigs.rs <- lmer(weight ~ week + (week|id), data=pigs)
r.squaredGLMM(pigs.rs) %>% round(2) %>%  pander()
```

`R2m` refers to the marginal $R^2$, `R2c` refers to the conditional value.

This means that, without the random intercepts, we could explain about 93% of the variance. With random intercepts we can explain over 98%. With 
random slopes it's over 99% of the variance.

One lesson here: Piglet growth rates are quite simple to explain. You don't often get models that fit this well with psychological data!

`r unhide()`

:::






## Likelihood ratio tests


[Wikipedia](https://en.wikipedia.org/wiki/Likelihood-ratio_test) helpfully defines the likelihood ratio test for us:

> In statistics, a likelihood ratio test (LR test) is a statistical test used for comparing the goodness of fit of two statistical models — a null model against an alternative model. The test is based on the likelihood ratio, which expresses how many times more likely the data are under one model than the other. This likelihood ratio ... can then be used to compute a *p*-value ... to decide whether to reject the null model.


In R, the `anova` function can be used to compare two models of the same type. We just write:

```{r, eval=F}
anova(model1, model2)
```

- For standard regression, this will produce an *F* test

- For mixed models (if you have loaded the `lmerTest` package), the `anova` function will actually returns a $\chi^2$ statistic from the likelihood ratio test.

In both cases the interpretation is the same: The test says whether the extra predictors in the more *complex* model explain a significant amount of *extra* variation.

- A significant *p* value (e.g. < .01) tells us than the more complex model provides a significantly better fit to the data (actually, that the simpler model is significantly *worse*).

- A non-significant *p* value tells us that the extra complexity is not worth the bother and we should prefer the simpler model.

***The big caveat here is that p values tell you that you can't reject the null, which isn't the same thing as rejecting an experimental hypothesis. Use Bayes factors to express evidence for or against a hypothesis of interest.***



:::{.exercise}

1. Use the `anova` command to compare the random intercept and random slope models.

`r hide("Show the code")`



```{r}
anova(pigs.ri, pigs.rs)
```

`r unhide()`




2. How do you interpret this output? Does the more complex random slope model provide a significantly better fit?


`r hide("Check answer")`

Yes - it does. The *p* value is very small, and may be showin in [scientific notation](https://en.wikipedia.org/wiki/Scientific_notation) (e.g. `<2e-16`).

The likelihood ratio test is telling us that the random slopes model explains significantly more variation in the outcome.

Remember though: Just because this is *statistically* significant it does not mean it's of any *practical* importance. Whether that is very meaningful will depend on the context, the sample size and the size of the effect.


`r unhide()`

:::


#### Reporting a LR test in prose

To report the test above you could write something like:

> The random intercept model was significantly improved by adding a random slope for  weeks, χ2(2) = 291.93, p < .001.

Or, if you plan to share your models code in a data supplement, you migth like to focus more on the interpretation rather than emphasising how the test was calculated and say:

> There was substantial variation in growth rates between-piglets,  χ2(2) = 291.93, p < .001.


At this point you would probably also want to use $R^2$ to quantify how much variation is being explained in each case (the conditional $R^2$).






#### A shortcut for testing random effects

As mentioned in class, the `ranova` function is a shortcut for testing random effects in an `lmer` model. To test if the random intercepts explain additional variance run:

```{r}
ranova(pigs.ri)
```

This shows the comparison of the random intercepts model against a simpler model which ignores clustering by-piglet. We can clearly see that piglets do differ from one another (the p value is very small).


This is the same test for the random *slopes*:


```{r}
ranova(pigs.rs)
```


You will see a column in the output called `LRT` and a corresponding *p* value for the $\chi^2$ test. You also see AIC values, which are an alternative way of comparing the models popular in some disciplines. See this stackoverflow post [@232494] for a nice summary of @burnham2004multimodel. A difference in AIC values between models > 3 or 4 is a good sign that the more complex model is justified.




# Scottish school leavers data {#school-leavers-longer-exercise}

Combine the techniques you have used above to re-analyse the Scottish school leavers dataset we saw in session 1.
In the data there is a variable called `cohort90`. This represents which year the data were sampled in relative to 1990: a value of `-6` means the data were sampled in 1984.


:::{.exercise}

- Read in the data from ([this file](data/cmm/5.1.txt))

- Plot a graph to explore whether there have been changes in `score` across cohorts

- Plot a graph to show change for 20 individual schools (remember that you need to group and summarise the data first to get the average for each school in each cohort; see hint below for some tricks we have not covered before). What pattern do you see in the plot?

- Fit a model including random intercepts for schools, and a fixed effect (slope) for cohort

- Fit another model, allowing for variation in the rate of change between schools (random slopes model)

- Is there significant variation in the rate of change between schools? How do you know?

- How much variance is explained by the preferred model?


`r hide("Show hint for one of the plots above")`

To make the second plot requested above you need to pre-process the data.
In the code below I:

- Create a list of schools we want to sample using the `sample` and `unique` functions
- Filter the dataset, using the `%in%` operator (there are other ways, but this is a neat one)
- Group by school and cohort to then
- Summarise (calculate the mean)


```{r}
schools <- read_csv('data/cmm/5.1.txt')
schools_to_sample <- sample(schools$schoolid %>% unique, 20)
schools %>%
  filter(schoolid %in% schools_to_sample) %>%  
  group_by(schoolid, cohort90) %>%
  summarise(score=mean(score))
```

`r unhide()`


:::

```{r, echo=F, include=F}
schools <- read_csv('data/cmm/5.1.txt')
schools %>% ggplot(aes(factor(cohort90), score)) + geom_boxplot()
schools %>%
  filter(schoolid %in% sample(schools$schoolid %>% unique, 20)) %>%  
  group_by(schoolid, cohort90) %>%
  summarise(score=mean(score)) %>%
  arrange(schoolid, cohort90) %>%
  ggplot(aes(cohort90, score, color=factor(schoolid))) + geom_line()


ssl.ri <- lmer(score ~ female + (1|schoolid), data=schools)
ssl.rs <- lmer(score ~ cohort90 + (cohort90|schoolid), data=schools)

ranova(ssl.rs)
```






<!--

Extra stuff for slides


```{r}
m1 <- lmer(weight ~ week + (week|id), data=pigs)
```

```{r}
m2 <- lmer(weight ~ week + (week|id), data=pigs %>% mutate(week=week-mean(week)))
```

```{r}
m3 <- lmer(weight ~ week + (week|id), data=pigs %>% mutate(week=week-max(week)))
```
```{r}
lmerModel <- m1
vpc <- function(lmerModel) {
  lmerModel %>%
    VarCorr() %>%
    as_tibble() %>%
    filter((str_detect(var1, "Interce")|grp=="Residual") & is.na(var2)) %>% 
    mutate(vpc = vcov/sum(vcov) * 100, 1) 
}
m1 %>% vpc
```



```{r}
m1 %>% vpc %>% pander
m2 %>% vpc %>% pander
m3 %>% vpc %>% pander
```

```{r}
pigs %>% 
  filter(id %in% 1:10) %>% 
  ggplot(aes(week, weight, group=id, color=factor(id))) + 
  geom_smooth(method=lm, se=F, alpha=.2, size=.2) + 
  geom_point()
ggsave('images/pigs-lines.png', width=4, height=3)
```



```{r}
tibble(residual=m1 %>% residuals) %>% 
  ggplot(aes(residual)) + geom_density()
ggsave('images/pigs-resid.png', width=4, height=3)


tibble(residual=m1 %>% residuals) %>% 
  ggplot(aes(sample=residual)) + geom_qq_line() + geom_qq()
ggsave('images/pigs-resid-qq.png', width=4, height=3)

require("lattice")
qqmath(m1, id=0.05) 

plot(m1)
```





```{r,include=T}
library(MASS)
e1.sd <- 1 #the standard deviation of the error term in the relationship y~a+b*x+e1
e2.sd <- 1 #the standard deviation of the error term in the relationship z~c+d*y+e2

e.cor <- matrix(c(1,0.3,0.3,1),2)
e.cov <- e.cor*as.matrix(c(e1.sd,e2.sd)) %*% t(as.matrix(c(e1.sd,e2.sd))) # the covarianc

mvrnorm(n = 3000, rep(0, 2), e.cov) 
  
  
set.seed(1)
d1 <- tibble(intercept= seq(-0.4,.8, .02)) %>% 
  mutate(slope=intercept+rnorm(n(), 0, .1)) 
d1 %>% 
  ggplot(aes(slope, intercept)) +
    geom_abline(aes(color=factor(slope), intercept=intercept, slope=slope)) + 
  coord_cartesian(xlim=c(0,1), ylim=c(-.4,1)) + 
  theme(legend.position = "none") 
    
ggsave('images/fan.png', width=4, height=4)

d1 %>% ggplot(aes(slope, intercept)) + geom_point()
ggsave('images/fan-corr.png', width=4, height=4)


d2 <- tibble(intercept= seq(-0.4,1, .05)) %>% 
  mutate(slope=intercept+rnorm(n(), 0, .1)) 
d2 %>% 
  ggplot(aes(slope, intercept)) +
  geom_abline(aes(color=factor(slope), intercept=intercept, slope=-slope)) + 
  coord_cartesian(xlim=c(0,1), ylim=c(-.5,1)) + 
  theme(legend.position = "none") +
  scale_x_continuous(labels=seq(0,1,.25))
    
ggsave('images/fan-neg.png', width=4, height=4)

d2 %>% ggplot(aes(slope, -intercept)) + geom_point()
ggsave('images/fan-neg-corr.png', width=4, height=4)

```

```{r}
set.seed(1)
d3 <- tibble(intercept= seq(-0.2,.4, .02)) %>% 
  mutate(slope=intercept+rnorm(n(), 0, .1)) 
d3 %>% 
  ggplot() +
    geom_abline(aes(color=factor(slope), intercept=intercept, slope=slope)) + 
  coord_cartesian(xlim=c(0,1), ylim=c(-.4,1)) + 
  theme(legend.position = "none") +
  geom_vline(aes(xintercept=0)) + 
  xlab("Years")

ggsave('images/slopes-zero.png', width=4, height = 3)    
```

```{r}

d3 %>% 
  ggplot() +
    geom_abline(aes(color=factor(slope), intercept=intercept, slope=slope)) + 
  coord_cartesian(xlim=c(0,1), ylim=c(-.4,1)) + 
  theme(legend.position = "none") +
  geom_vline(aes(xintercept=.5)) + 
  scale_x_continuous(labels=c(-.5,-.25,0,.25,.5)) +
  xlab("Years")
ggsave('images/slopes-mean.png', width=4, height = 3)        
```


```{r}
d3 %>% 
  ggplot() +
    geom_abline(aes(color=factor(slope), intercept=intercept, slope=slope)) + 
  coord_cartesian(xlim=c(0,1), ylim=c(-.4,1)) + 
  theme(legend.position = "none") +
  geom_vline(aes(xintercept=1)) + 
  scale_x_continuous(labels=c(-.5,-.25,0,.25,.5)-.5) +
  xlab("Years")
ggsave('images/slopes-end.png', width=4, height = 3)    
    

```




-->



# References